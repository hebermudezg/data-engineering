{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69b5a3b0-59b7-42ff-bfb0-d6d5fb0bad5f",
   "metadata": {},
   "source": [
    "# PySpark \n",
    "----\n",
    "PySpark is an interface for Apache Spark in Python. It not only allows you to write Spark applications using Python APIs, but also provides the PySpark shell for interactively analyzing your data in a distributed environment. PySpark supports most of Sparkâ€™s features such as Spark SQL, DataFrame, Streaming, MLlib (Machine Learning) and Spark Core.\n",
    "\n",
    "PySpark DataFrames are lazily evaluated. They are implemented on top of RDDs. When Spark transforms data, it does not immediately compute the transformation but plans how to compute later. When actions such as collect() are explicitly called, the computation starts. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4cc2e7-f179-4e90-8279-b07d61d63552",
   "metadata": {},
   "source": [
    "## Installation\n",
    "\n",
    "First we have to install pyspark as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d22fb70-504d-4ab2-9eaf-9d45aaaf49fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in /Users/hebermudezg/anaconda3/lib/python3.11/site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in /Users/hebermudezg/anaconda3/lib/python3.11/site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9624ca3-72ef-48ab-ab0e-386253f2913c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1423e845-ffc0-4e4d-a0e1-3810b88fc1e3",
   "metadata": {
    "tags": []
   },
   "source": [
    "PySpark applications start with initializing SparkSession which is the entry point of PySpark as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5bf8ada-4286-4346-8558-9c3048e2814f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/30 23:03:38 WARN Utils: Your hostname, Hebers-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.5 instead (on interface en0)\n",
      "23/09/30 23:03:38 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/30 23:03:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Creating SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578836dc-c455-46db-9a5e-16d276e3a057",
   "metadata": {},
   "source": [
    "When you work with Spark in local mode (i.e., not in a distributed cluster), Spark runs on your local machine using all available CPU cores by default. If you want to configure the number of cores used, you can do so when creating a Spark session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cff67593-0b82-4b50-8d72-b0123d050b59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the number of cores used\n",
    "spark.sparkContext.getConf().get(\"spark.master\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0b59ea-2767-4ae1-9664-e38a83f8edda",
   "metadata": {},
   "source": [
    "If you see a value like `local[*]`, it means Spark is using all available cores. If you see `local[4]`, it means Spark is using 4 cores.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApplication\") \\\n",
    "    .master(\"local[2]\")  # Uses 2 cores\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9c8244b-7a77-4866-b6d8-475e450130be",
   "metadata": {},
   "source": [
    "## Data Frame Creation\n",
    "\n",
    "You can create PySpark DataFrame from a list of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbc4a934-eab7-4fb6-9336-fd4826d8c971",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f695e9b9-5650-41ea-9dd6-e60c4e8b9d9a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    Row(a=1, b=2., c='string1', d=date(2000, 1, 1), e=datetime(2000, 1, 1, 12, 0)),\n",
    "    Row(a=2, b=3., c='string2', d=date(2000, 2, 1), e=datetime(2000, 1, 2, 12, 0)),\n",
    "    Row(a=4, b=5., c='string3', d=date(2000, 3, 1), e=datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868918b2-acf6-4841-92e6-d580a9d880cd",
   "metadata": {},
   "source": [
    "Create a PySpark DataFrame with an explicit schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e17e790-6ce5-499f-9954-94a747f53979",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "], schema='a long, b double, c string, d date, e timestamp')\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edff8fa-efe0-41ce-b10b-2a896d50dd45",
   "metadata": {},
   "source": [
    "Create a PySpark DataFrame from a pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9950c3c5-19e3-402b-b760-04326ca9d2fc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pandas_df = pd.DataFrame({\n",
    "    'a': [1, 2, 3],\n",
    "    'b': [2., 3., 4.],\n",
    "    'c': ['string1', 'string2', 'string3'],\n",
    "    'd': [date(2000, 1, 1), date(2000, 2, 1), date(2000, 3, 1)],\n",
    "    'e': [datetime(2000, 1, 1, 12, 0), datetime(2000, 1, 2, 12, 0), datetime(2000, 1, 3, 12, 0)]\n",
    "})\n",
    "df = spark.createDataFrame(pandas_df)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4782d902-fb13-475f-85dd-df12bb8102b0",
   "metadata": {},
   "source": [
    "Create a PySpark DataFrame from an RDD consisting of a list of tuples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7469542e-8dc5-435a-9438-7e2243c5cf94",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = spark.sparkContext.parallelize([\n",
    "    (1, 2., 'string1', date(2000, 1, 1), datetime(2000, 1, 1, 12, 0)),\n",
    "    (2, 3., 'string2', date(2000, 2, 1), datetime(2000, 1, 2, 12, 0)),\n",
    "    (3, 4., 'string3', date(2000, 3, 1), datetime(2000, 1, 3, 12, 0))\n",
    "])\n",
    "df = spark.createDataFrame(rdd, schema=['a', 'b', 'c', 'd', 'e'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607c141a-6247-4b68-998d-f72789d1e5f0",
   "metadata": {},
   "source": [
    "## Show and Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "104652cc-ea24-49eb-99e1-d006e62ddc92",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n",
      "root\n",
      " |-- a: long (nullable = true)\n",
      " |-- b: double (nullable = true)\n",
      " |-- c: string (nullable = true)\n",
      " |-- d: date (nullable = true)\n",
      " |-- e: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# All DataFrames above result same.\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59cfaad9-5668-467b-ad81-fe1af5e44e1d",
   "metadata": {},
   "source": [
    "## Viewing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71f3ccd5-20dc-462e-be62-8ca0dd280837",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#The top rows of a DataFrame can be displayed using DataFrame.show().\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905e22b4-0044-4a79-9080-5b9338889d23",
   "metadata": {},
   "source": [
    "Alternatively, you can enable spark.sql.repl.eagerEval.enabled configuration for the eager evaluation of PySpark DataFrame in notebooks such as Jupyter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc5ea04d-317d-4a1c-b52d-b6acf75e8376",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border='1'>\n",
       "<tr><th>a</th><th>b</th><th>c</th><th>d</th><th>e</th></tr>\n",
       "<tr><td>1</td><td>2.0</td><td>string1</td><td>2000-01-01</td><td>2000-01-01 12:00:00</td></tr>\n",
       "<tr><td>2</td><td>3.0</td><td>string2</td><td>2000-02-01</td><td>2000-01-02 12:00:00</td></tr>\n",
       "<tr><td>3</td><td>4.0</td><td>string3</td><td>2000-03-01</td><td>2000-01-03 12:00:00</td></tr>\n",
       "</table>\n"
      ],
      "text/plain": [
       "DataFrame[a: bigint, b: double, c: string, d: date, e: timestamp]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.conf.set('spark.sql.repl.eagerEval.enabled', True)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2d8d8b-983f-4d7f-ba9c-5bb2fb401d3b",
   "metadata": {},
   "source": [
    "The rows can also be show vertically. This is usefull when rows are too long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f5059fc-aac5-4169-a65f-fecdddf3cd0d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-RECORD 0------------------\n",
      " a   | 1                   \n",
      " b   | 2.0                 \n",
      " c   | string1             \n",
      " d   | 2000-01-01          \n",
      " e   | 2000-01-01 12:00:00 \n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0cc233-7908-436c-9639-6a130e56d645",
   "metadata": {},
   "source": [
    "We can see columns names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59d2d835-b002-4024-8c51-ebbe13fdf546",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'b', 'c', 'd', 'e']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581c176e-4b0c-4a99-9e8c-14e7190b350a",
   "metadata": {
    "tags": []
   },
   "source": [
    "Show the summary of the Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0d311eaf-837b-48b7-9007-bd03597957a8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+---+-------+\n",
      "|summary|  a|  b|      c|\n",
      "+-------+---+---+-------+\n",
      "|  count|  3|  3|      3|\n",
      "|   mean|2.0|3.0|   NULL|\n",
      "| stddev|1.0|1.0|   NULL|\n",
      "|    min|  1|2.0|string1|\n",
      "|    max|  3|4.0|string3|\n",
      "+-------+---+---+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.select('a', 'b', 'c').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9432667-7713-4495-8b72-d2e58351c668",
   "metadata": {},
   "source": [
    "DataFrame.collect() collects the distributed data to the driver side as the local data in Python. Note that this can throw an out-of-memory error when the dataset is too large to fit in the driver side because it collects all the data from executors to the driver side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dfb817f-8ec6-471e-b2ff-90dcfae8c836",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0)),\n",
       " Row(a=2, b=3.0, c='string2', d=datetime.date(2000, 2, 1), e=datetime.datetime(2000, 1, 2, 12, 0)),\n",
       " Row(a=3, b=4.0, c='string3', d=datetime.date(2000, 3, 1), e=datetime.datetime(2000, 1, 3, 12, 0))]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9757960b-8df3-42ff-bff3-33f77bcba594",
   "metadata": {},
   "source": [
    "In order to avoid throwing an out-of-memory exception, use DataFrame.take() or DataFrame.tail()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "851805b8-3c82-41c4-b843-bf05f8c81e25",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(a=1, b=2.0, c='string1', d=datetime.date(2000, 1, 1), e=datetime.datetime(2000, 1, 1, 12, 0))]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16d75da-627a-4713-a425-94c52c9a6d8f",
   "metadata": {},
   "source": [
    "PySpark DataFrame also provides the conversion back to a pandas DataFrame to leverage pandas API. Note that toPandas also collects all data into the driver side that can easily cause an out-of-memory-error when the data is too large to fit into the driver side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7e0726b7-50ba-4a5e-913e-7efd24c7fdeb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "      <th>c</th>\n",
       "      <th>d</th>\n",
       "      <th>e</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>string1</td>\n",
       "      <td>2000-01-01</td>\n",
       "      <td>2000-01-01 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>string2</td>\n",
       "      <td>2000-02-01</td>\n",
       "      <td>2000-01-02 12:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>string3</td>\n",
       "      <td>2000-03-01</td>\n",
       "      <td>2000-01-03 12:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   a    b        c           d                   e\n",
       "0  1  2.0  string1  2000-01-01 2000-01-01 12:00:00\n",
       "1  2  3.0  string2  2000-02-01 2000-01-02 12:00:00\n",
       "2  3  4.0  string3  2000-03-01 2000-01-03 12:00:00"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a399865-a6eb-4541-8e45-4ec3fa584619",
   "metadata": {},
   "source": [
    "## Selecting and Accessing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412b7328-5140-410b-ac9a-a1e6681d929f",
   "metadata": {},
   "source": [
    "PySpark DataFrame is lazily evaluated and simply selecting a column does not trigger the computation but it returns a Column instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68798a78-99d3-4090-bfa7-ffec5d9b00b9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'a'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6557ec8f-6373-42f1-af32-17c52c5418b4",
   "metadata": {},
   "source": [
    "These Columns can be used to select the columns from a DataFrame. For example, DataFrame.select() takes the Column instances that returns another DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4fc96163-3096-41b0-a494-f049f966f2dc",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+\n",
      "|      c|\n",
      "+-------+\n",
      "|string1|\n",
      "|string2|\n",
      "|string3|\n",
      "+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(df.c).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cace2c9a-5dd2-4e9b-b764-b6810714fbad",
   "metadata": {},
   "source": [
    "Assign new Column instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f6ecd7fd-3809-48ba-8bfb-0a5f08ceafcf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+-------+\n",
      "|  a|  b|      c|         d|                  e|upper_c|\n",
      "+---+---+-------+----------+-------------------+-------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|STRING1|\n",
      "|  2|3.0|string2|2000-02-01|2000-01-02 12:00:00|STRING2|\n",
      "|  3|4.0|string3|2000-03-01|2000-01-03 12:00:00|STRING3|\n",
      "+---+---+-------+----------+-------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import upper\n",
    "df.withColumn('upper_c', upper(df.c)).show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288a661b-bf2c-4583-8395-f089ccd86641",
   "metadata": {},
   "source": [
    "To select a subset of rows, use DataFrame.filter()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ec603896-ab96-4c6a-9265-723501cada79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+---+-------+----------+-------------------+\n",
      "|  a|  b|      c|         d|                  e|\n",
      "+---+---+-------+----------+-------------------+\n",
      "|  1|2.0|string1|2000-01-01|2000-01-01 12:00:00|\n",
      "+---+---+-------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.a == 1).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe7edb2-6f99-4c25-bac2-4162b62ff944",
   "metadata": {},
   "source": [
    "## Applying a Function\n",
    "\n",
    "PySpark supports various UDFs and APIs to allow users to execute Python native functions. See also the latest Pandas UDFs and Pandas Function APIs. For instance, the example below allows users to directly use the APIs in a pandas Series within Python native function.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "384070c2-7707-44b0-9612-9aa540cec8e9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|pandas_plus_one(a)|\n",
      "+------------------+\n",
      "|                 2|\n",
      "|                 3|\n",
      "|                 4|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pyspark.sql.functions import pandas_udf\n",
    "\n",
    "@pandas_udf('long')\n",
    "def pandas_plus_one(series: pd.Series) -> pd.Series:\n",
    "    # Simply plus one by using pandas Series.\n",
    "    return series + 1\n",
    "\n",
    "df.select(pandas_plus_one(df.a)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88283279-93b8-4ef7-bacb-b96975120d81",
   "metadata": {},
   "source": [
    "## Grouping Data\n",
    "\n",
    "PySpark DataFrame also provides a way of handling grouped data by using the common approach, split-apply-combine strategy. It groups the data by a certain condition applies a function to each group and then combines them back to the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21b65860-2a3b-4c36-8821-0d52c64d5387",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|  red|banana|  1| 10|\n",
      "| blue|banana|  2| 20|\n",
      "|  red|carrot|  3| 30|\n",
      "| blue| grape|  4| 40|\n",
      "|  red|carrot|  5| 50|\n",
      "|black|carrot|  6| 60|\n",
      "|  red|banana|  7| 70|\n",
      "|  red| grape|  8| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    ['red', 'banana', 1, 10], ['blue', 'banana', 2, 20], ['red', 'carrot', 3, 30],\n",
    "    ['blue', 'grape', 4, 40], ['red', 'carrot', 5, 50], ['black', 'carrot', 6, 60],\n",
    "    ['red', 'banana', 7, 70], ['red', 'grape', 8, 80]], schema=['color', 'fruit', 'v1', 'v2'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae91ec8-9c6e-4bb0-9197-6a27f3a9d21d",
   "metadata": {},
   "source": [
    "Grouping and then applying the avg() function to the resulting groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f0e8e22-7765-4931-8591-19f3174725de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:>                                                         (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+-------+\n",
      "|color|avg(v1)|avg(v2)|\n",
      "+-----+-------+-------+\n",
      "|  red|    4.8|   48.0|\n",
      "| blue|    3.0|   30.0|\n",
      "|black|    6.0|   60.0|\n",
      "+-----+-------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df.groupby('color').avg().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce452164-fa68-4f02-9117-03199c458391",
   "metadata": {},
   "source": [
    "You can also apply a Python native function against each group by using pandas API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2c4b6f5d-bf24-4845-bb00-48630b25328f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|black|carrot|  0| 60|\n",
      "| blue|banana| -1| 20|\n",
      "| blue| grape|  1| 40|\n",
      "|  red|banana| -3| 10|\n",
      "|  red|carrot| -1| 30|\n",
      "|  red|carrot|  0| 50|\n",
      "|  red|banana|  2| 70|\n",
      "|  red| grape|  3| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def plus_mean(pandas_df):\n",
    "    return pandas_df.assign(v1=pandas_df.v1 - pandas_df.v1.mean())\n",
    "\n",
    "df.groupby('color').applyInPandas(plus_mean, schema=df.schema).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f567eb81-5f17-4e4e-b0be-eec4e56a4405",
   "metadata": {},
   "source": [
    "Co-grouping and applying a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ce811b81-6b1f-457a-8472-310c8a39a4ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+---+---+\n",
      "|    time| id| v1| v2|\n",
      "+--------+---+---+---+\n",
      "|20000101|  1|1.0|  x|\n",
      "|20000102|  1|3.0|  x|\n",
      "|20000101|  2|2.0|  y|\n",
      "|20000102|  2|4.0|  y|\n",
      "+--------+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1 = spark.createDataFrame(\n",
    "    [(20000101, 1, 1.0), (20000101, 2, 2.0), (20000102, 1, 3.0), (20000102, 2, 4.0)],\n",
    "    ('time', 'id', 'v1'))\n",
    "\n",
    "df2 = spark.createDataFrame(\n",
    "    [(20000101, 1, 'x'), (20000101, 2, 'y')],\n",
    "    ('time', 'id', 'v2'))\n",
    "\n",
    "def asof_join(l, r):\n",
    "    return pd.merge_asof(l, r, on='time', by='id')\n",
    "\n",
    "df1.groupby('id').cogroup(df2.groupby('id')).applyInPandas(\n",
    "    asof_join, schema='time int, id int, v1 double, v2 string').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0545ee16-7fe7-42c4-8def-da1e56e7092e",
   "metadata": {},
   "source": [
    "## Getting Data in/out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "67394a2d-9872-46b5-9311-56e2466a1701",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+---+---+\n",
      "|color| fruit| v1| v2|\n",
      "+-----+------+---+---+\n",
      "|black|carrot|  6| 60|\n",
      "| blue|banana|  2| 20|\n",
      "|  red|banana|  7| 70|\n",
      "|  red|carrot|  5| 50|\n",
      "|  red|carrot|  3| 30|\n",
      "| blue| grape|  4| 40|\n",
      "|  red|banana|  1| 10|\n",
      "|  red| grape|  8| 80|\n",
      "+-----+------+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.write.csv('foo.csv', header=True)\n",
    "spark.read.csv('foo.csv', header=True).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ed13e-dfe4-4a5c-becb-6807bff33113",
   "metadata": {},
   "source": [
    "## Working with SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8582a497-e36a-47ae-876e-3841cf4a0dff",
   "metadata": {
    "tags": []
   },
   "source": [
    "DataFrame and Spark SQL share the same execution engine so they can be interchangeably used seamlessly. For example, you can register the DataFrame as a table and run a SQL easily as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8cff5a53-7567-4b76-aae2-e101d3f56417",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       8|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"tableA\")\n",
    "spark.sql(\"SELECT count(*) from tableA\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa73bc74-7bd8-4bd2-8a08-147974797e41",
   "metadata": {},
   "source": [
    "In addition, UDFs can be registered and invoked in SQL out of the box:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9c0bc9ec-2170-41e2-830f-d1430a826b20",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|add_one(v1)|\n",
      "+-----------+\n",
      "|          2|\n",
      "|          3|\n",
      "|          4|\n",
      "|          5|\n",
      "|          6|\n",
      "|          7|\n",
      "|          8|\n",
      "|          9|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "@pandas_udf(\"integer\")\n",
    "def add_one(s: pd.Series) -> pd.Series:\n",
    "    return s + 1\n",
    "\n",
    "spark.udf.register(\"add_one\", add_one)\n",
    "spark.sql(\"SELECT add_one(v1) FROM tableA\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2632ffe-4692-4584-96dc-5df23dadc7a4",
   "metadata": {},
   "source": [
    "These SQL expressions can directly be mixed and used as PySpark columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c6892e3e-b68b-41e5-8dcb-e252ea00989e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|add_one(v1)|\n",
      "+-----------+\n",
      "|          2|\n",
      "|          3|\n",
      "|          4|\n",
      "|          5|\n",
      "|          6|\n",
      "|          7|\n",
      "|          8|\n",
      "|          9|\n",
      "+-----------+\n",
      "\n",
      "+--------------+\n",
      "|(count(1) > 0)|\n",
      "+--------------+\n",
      "|          true|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "df.selectExpr('add_one(v1)').show()\n",
    "df.select(expr('count(*)') > 0).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8574c78-9ea2-45a9-ab0f-c4dffe68bbf1",
   "metadata": {},
   "source": [
    "## Practical Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17124c00-666e-4382-8f9a-9e6cb4d15c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Reading data.\n",
    "df = spark.read.csv('titanic_train.csv', header = True,\n",
    "                    inferSchema= True) # InferSchema so as to get the correct datatype\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "786c6de6-e0a7-4bda-ac5c-c9a11cd6f871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing the type of this object. \n",
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "12fb8039-0d8f-4ea5-a719-91bc346e7809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Schema\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aaa72293-703f-41d4-9888-267f31faff8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PassengerId',\n",
       " 'Survived',\n",
       " 'Pclass',\n",
       " 'Name',\n",
       " 'Sex',\n",
       " 'Age',\n",
       " 'SibSp',\n",
       " 'Parch',\n",
       " 'Ticket',\n",
       " 'Fare',\n",
       " 'Cabin',\n",
       " 'Embarked']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "3faa28b2-38fe-48cf-a04d-510e93201416",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+\n",
      "|Survived|                Name|\n",
      "+--------+--------------------+\n",
      "|       0|Braund, Mr. Owen ...|\n",
      "|       1|Cumings, Mrs. Joh...|\n",
      "|       1|Heikkinen, Miss. ...|\n",
      "|       1|Futrelle, Mrs. Ja...|\n",
      "|       0|Allen, Mr. Willia...|\n",
      "|       0|    Moran, Mr. James|\n",
      "|       0|McCarthy, Mr. Tim...|\n",
      "|       0|Palsson, Master. ...|\n",
      "|       1|Johnson, Mrs. Osc...|\n",
      "|       1|Nasser, Mrs. Nich...|\n",
      "|       1|Sandstrom, Miss. ...|\n",
      "|       1|Bonnell, Miss. El...|\n",
      "|       0|Saundercock, Mr. ...|\n",
      "|       0|Andersson, Mr. An...|\n",
      "|       0|Vestrom, Miss. Hu...|\n",
      "|       1|Hewlett, Mrs. (Ma...|\n",
      "|       0|Rice, Master. Eugene|\n",
      "|       1|Williams, Mr. Cha...|\n",
      "|       0|Vander Planke, Mr...|\n",
      "|       1|Masselmani, Mrs. ...|\n",
      "+--------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# How to select columns\n",
    "df.select(['Survived', 'Name']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3f254b06-2c83-444d-91dd-f234382d1862",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PassengerId', 'int'),\n",
       " ('Survived', 'int'),\n",
       " ('Pclass', 'int'),\n",
       " ('Name', 'string'),\n",
       " ('Sex', 'string'),\n",
       " ('Age', 'double'),\n",
       " ('SibSp', 'int'),\n",
       " ('Parch', 'int'),\n",
       " ('Ticket', 'string'),\n",
       " ('Fare', 'double'),\n",
       " ('Cabin', 'string'),\n",
       " ('Embarked', 'string')]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Showing data types\n",
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "262e4706-f0f1-486a-9498-c56b656c0b50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/09/30 23:22:51 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 71:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|summary|      PassengerId|           Survived|            Pclass|                Name|   Sex|               Age|             SibSp|              Parch|            Ticket|             Fare|Cabin|Embarked|\n",
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "|  count|              891|                891|               891|                 891|   891|               714|               891|                891|               891|              891|  204|     889|\n",
      "|   mean|            446.0| 0.3838383838383838| 2.308641975308642|                NULL|  NULL| 29.69911764705882|0.5230078563411896|0.38159371492704824|260318.54916792738| 32.2042079685746| NULL|    NULL|\n",
      "| stddev|257.3538420152301|0.48659245426485753|0.8360712409770491|                NULL|  NULL|14.526497332334035|1.1027434322934315| 0.8060572211299488|471609.26868834975|49.69342859718089| NULL|    NULL|\n",
      "|    min|                1|                  0|                 1|\"Andersson, Mr. A...|female|              0.42|                 0|                  0|            110152|              0.0|  A10|       C|\n",
      "|    max|              891|                  1|                 3|van Melkebeke, Mr...|  male|              80.0|                 8|                  6|         WE/P 5735|         512.3292|    T|       S|\n",
      "+-------+-----------------+-------------------+------------------+--------------------+------+------------------+------------------+-------------------+------------------+-----------------+-----+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Describe or summary \n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e049a267-cb9a-42c3-b502-8c84b6113b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding columns in PySpark df\n",
    "df = df.withColumn('Converted Fare', df['Fare'] * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "06247375-12e2-477d-abd4-c79236215e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+--------------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|Converted Fare|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+--------------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|          14.5|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|      142.5666|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|         15.85|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|         106.2|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|          16.1|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+--------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a668c88a-2c37-4b28-8926-00134b9cf162",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping columns\n",
    "df = df.drop('Converted Fare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b202bbd3-825c-4bf6-9ff0-665e9b1c0ae3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "4a74eb55-bbc4-4df9-bf87-030554ae8f0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|           Full Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Renaming Columns\n",
    "df.withColumnRenamed('Name', 'Full Name').show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0d0eb07f-220f-49e4-a737-37852f574691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+-----------+--------+-----------+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|     Ticket|    Fare|      Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+-----------+--------+-----------+--------+\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|   PC 17599| 71.2833|        C85|       C|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|     113803|    53.1|       C123|       S|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|      17463| 51.8625|        E46|       S|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|    PP 9549|    16.7|         G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|     113783|   26.55|       C103|       S|\n",
      "|         22|       1|     2|Beesley, Mr. Lawr...|  male|34.0|    0|    0|     248698|    13.0|        D56|       S|\n",
      "|         24|       1|     1|Sloper, Mr. Willi...|  male|28.0|    0|    0|     113788|    35.5|         A6|       S|\n",
      "|         28|       0|     1|Fortune, Mr. Char...|  male|19.0|    3|    2|      19950|   263.0|C23 C25 C27|       S|\n",
      "|         53|       1|     1|Harper, Mrs. Henr...|female|49.0|    1|    0|   PC 17572| 76.7292|        D33|       C|\n",
      "|         55|       0|     1|Ostby, Mr. Engelh...|  male|65.0|    0|    1|     113509| 61.9792|        B30|       C|\n",
      "|         63|       0|     1|Harris, Mr. Henry...|  male|45.0|    1|    0|      36973|  83.475|        C83|       S|\n",
      "|         67|       1|     2|Nye, Mrs. (Elizab...|female|29.0|    0|    0| C.A. 29395|    10.5|        F33|       S|\n",
      "|         76|       0|     3|Moen, Mr. Sigurd ...|  male|25.0|    0|    0|     348123|    7.65|      F G73|       S|\n",
      "|         89|       1|     1|Fortune, Miss. Ma...|female|23.0|    3|    2|      19950|   263.0|C23 C25 C27|       S|\n",
      "|         93|       0|     1|Chaffee, Mr. Herb...|  male|46.0|    1|    0|W.E.P. 5734|  61.175|        E31|       S|\n",
      "|         97|       0|     1|Goldschmidt, Mr. ...|  male|71.0|    0|    0|   PC 17754| 34.6542|         A5|       C|\n",
      "|         98|       1|     1|Greenfield, Mr. W...|  male|23.0|    0|    1|   PC 17759| 63.3583|    D10 D12|       C|\n",
      "|        103|       0|     1|White, Mr. Richar...|  male|21.0|    0|    1|      35281| 77.2875|        D26|       S|\n",
      "|        111|       0|     1|Porter, Mr. Walte...|  male|47.0|    0|    0|     110465|    52.0|       C110|       S|\n",
      "|        119|       0|     1|Baxter, Mr. Quigg...|  male|24.0|    0|    1|   PC 17558|247.5208|    B58 B60|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+-----------+--------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# To eliminate all observation or columns with NA, Null\n",
    "# By default, it will remove any row that contains at least one null value.\n",
    "df.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "42c2b12f-0c99-44fb-a75f-3ed361254e17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|NULL|    0|    0|          330877| 8.4583| NULL|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| NULL|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| NULL|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| NULL|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| NULL|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| NULL|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| NULL|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| NULL|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| NULL|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|NULL|    0|    0|          244373|   13.0| NULL|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| NULL|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|NULL|    0|    0|            2649|  7.225| NULL|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The \"thresh\" argument is used to set a threshold for the number of non-null values a row must have before it is deleted. \n",
    "# In this case, the threshold is set to 2, which means that a row must have at least two non-null values in order for \n",
    "# it not to be deleted.\n",
    "\n",
    "df.na.drop(how ='all', thresh=2).show() # There are thres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bac9a530-ed82-4267-853a-9fff16eab88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|NULL|    0|    0|          330877| 8.4583| NULL|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| NULL|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| NULL|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| NULL|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| NULL|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| NULL|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| NULL|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| NULL|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| NULL|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male|NULL|    0|    0|          244373|   13.0| NULL|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| NULL|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female|NULL|    0|    0|            2649|  7.225| NULL|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# This line of code removes the rows from the \"df\" DataFrame that have null values in the \"Survived\" column.\n",
    "df.na.drop(how='any', subset=['Survived']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a6ee72c6-a9e3-4a69-8fce-d06dec157f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|PassengerId|Survived|Pclass|Name|Sex|Age|SibSp|Parch|Ticket|Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "|          0|       0|     0|   0|  0|177|    0|    0|     0|   0|  687|       2|\n",
      "+-----------+--------+------+----+---+---+-----+-----+------+----+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import isnull, sum\n",
    "\n",
    "# Assuming `df` is the PySpark DataFrame you want to check\n",
    "null_counts = df.select([sum(isnull(c).cast(\"int\")).alias(c) for c in df.columns])\n",
    "null_counts.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "e2a90f39-988b-4fd9-a0a7-b43f062443e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male| 0.0|    0|    0|          330877| 8.4583| NULL|       Q|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| NULL|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| NULL|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| NULL|       C|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       S|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| NULL|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| NULL|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| NULL|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| NULL|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| NULL|       Q|\n",
      "|         18|       1|     2|Williams, Mr. Cha...|  male| 0.0|    0|    0|          244373|   13.0| NULL|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| NULL|       S|\n",
      "|         20|       1|     3|Masselmani, Mrs. ...|female| 0.0|    0|    0|            2649|  7.225| NULL|       C|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filling the missing values\n",
    "df.na.fill(0, ['Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "56800d44-948c-4224-8384-570fefc29a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "imputer = Imputer(\n",
    "    inputCols=['Age', 'Fare'],\n",
    "    outputCols=[\"{}_impute\".format(c) for c in ['Age', 'Fare']]).setStrategy('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d2861f1f-5146-40fc-8a1a-d1e0f619f3e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+-----------------+-----------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|       Age_impute|Fare_impute|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+-----------------+-----------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|             22.0|       7.25|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|             38.0|    71.2833|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| NULL|       S|             26.0|      7.925|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|             35.0|       53.1|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|             35.0|       8.05|\n",
      "|          6|       0|     3|    Moran, Mr. James|  male|NULL|    0|    0|          330877| 8.4583| NULL|       Q|29.69911764705882|     8.4583|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       S|             54.0|    51.8625|\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| NULL|       S|              2.0|     21.075|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+-----------------+-----------+\n",
      "only showing top 8 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df).transform(df).show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2d1980c4-5bf0-47d2-84f4-9cc79a6cea41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+-------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|       Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+-------------+-------+-----+--------+\n",
      "|          8|       0|     3|Palsson, Master. ...|  male| 2.0|    3|    1|       349909| 21.075| NULL|       S|\n",
      "|         11|       1|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|      PP 9549|   16.7|   G6|       S|\n",
      "|         17|       0|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|       382652| 29.125| NULL|       Q|\n",
      "|         25|       0|     3|Palsson, Miss. To...|female| 8.0|    3|    1|       349909| 21.075| NULL|       S|\n",
      "|         44|       1|     2|Laroche, Miss. Si...|female| 3.0|    1|    2|SC/Paris 2123|41.5792| NULL|       C|\n",
      "|         51|       0|     3|Panula, Master. J...|  male| 7.0|    4|    1|      3101295|39.6875| NULL|       S|\n",
      "|         59|       1|     2|West, Miss. Const...|female| 5.0|    1|    2|   C.A. 34651|  27.75| NULL|       S|\n",
      "|         64|       0|     3|Skoog, Master. Ha...|  male| 4.0|    3|    2|       347088|   27.9| NULL|       S|\n",
      "|         79|       1|     2|Caldwell, Master....|  male|0.83|    0|    2|       248738|   29.0| NULL|       S|\n",
      "|        120|       0|     3|Andersson, Miss. ...|female| 2.0|    4|    2|       347082| 31.275| NULL|       S|\n",
      "|        148|       0|     3|\"Ford, Miss. Robi...|female| 9.0|    2|    2|   W./C. 6608| 34.375| NULL|       S|\n",
      "|        165|       0|     3|Panula, Master. E...|  male| 1.0|    4|    1|      3101295|39.6875| NULL|       S|\n",
      "|        166|       1|     3|\"Goldsmith, Maste...|  male| 9.0|    0|    2|       363291| 20.525| NULL|       S|\n",
      "|        172|       0|     3|Rice, Master. Arthur|  male| 4.0|    4|    1|       382652| 29.125| NULL|       Q|\n",
      "|        173|       1|     3|Johnson, Miss. El...|female| 1.0|    1|    1|       347742|11.1333| NULL|       S|\n",
      "|        183|       0|     3|Asplund, Master. ...|  male| 9.0|    4|    2|       347077|31.3875| NULL|       S|\n",
      "|        184|       1|     2|Becker, Master. R...|  male| 1.0|    2|    1|       230136|   39.0|   F4|       S|\n",
      "|        185|       1|     3|Kink-Heilmann, Mi...|female| 4.0|    0|    2|       315153| 22.025| NULL|       S|\n",
      "|        194|       1|     2|Navratil, Master....|  male| 3.0|    1|    1|       230080|   26.0|   F2|       S|\n",
      "|        206|       0|     3|Strom, Miss. Telm...|female| 2.0|    0|    1|       347054|10.4625|   G6|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+-------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter Operation\n",
    "df.filter('Age < 10').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "55c3340a-94ae-4171-8145-79a633739c7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+\n",
      "|                Name| Age|\n",
      "+--------------------+----+\n",
      "|Palsson, Master. ...| 2.0|\n",
      "|Sandstrom, Miss. ...| 4.0|\n",
      "|Rice, Master. Eugene| 2.0|\n",
      "|Palsson, Miss. To...| 8.0|\n",
      "|Laroche, Miss. Si...| 3.0|\n",
      "|Panula, Master. J...| 7.0|\n",
      "|West, Miss. Const...| 5.0|\n",
      "|Skoog, Master. Ha...| 4.0|\n",
      "|Caldwell, Master....|0.83|\n",
      "|Andersson, Miss. ...| 2.0|\n",
      "|\"Ford, Miss. Robi...| 9.0|\n",
      "|Panula, Master. E...| 1.0|\n",
      "|\"Goldsmith, Maste...| 9.0|\n",
      "|Rice, Master. Arthur| 4.0|\n",
      "|Johnson, Miss. El...| 1.0|\n",
      "|Asplund, Master. ...| 9.0|\n",
      "|Becker, Master. R...| 1.0|\n",
      "|Kink-Heilmann, Mi...| 4.0|\n",
      "|Navratil, Master....| 3.0|\n",
      "|Strom, Miss. Telm...| 2.0|\n",
      "+--------------------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter Operation\n",
    "df.filter('Age < 10').select(['Name', 'Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "74c0b7ab-524a-4154-85cc-0bb9f688dc48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----+------+\n",
      "|                Name| Age|   Sex|\n",
      "+--------------------+----+------+\n",
      "|Sandstrom, Miss. ...| 4.0|female|\n",
      "|Palsson, Miss. To...| 8.0|female|\n",
      "|Laroche, Miss. Si...| 3.0|female|\n",
      "|West, Miss. Const...| 5.0|female|\n",
      "|Andersson, Miss. ...| 2.0|female|\n",
      "|\"Ford, Miss. Robi...| 9.0|female|\n",
      "|Johnson, Miss. El...| 1.0|female|\n",
      "|Kink-Heilmann, Mi...| 4.0|female|\n",
      "|Strom, Miss. Telm...| 2.0|female|\n",
      "|Asplund, Miss. Li...| 5.0|female|\n",
      "|\"Collyer, Miss. M...| 8.0|female|\n",
      "|Allison, Miss. He...| 2.0|female|\n",
      "|Palsson, Miss. St...| 3.0|female|\n",
      "|\"Nakid, Miss. Mar...| 1.0|female|\n",
      "|Baclini, Miss. Ma...| 5.0|female|\n",
      "|Baclini, Miss. He...|0.75|female|\n",
      "|Hirvonen, Miss. H...| 2.0|female|\n",
      "|Quick, Miss. Phyl...| 2.0|female|\n",
      "|Hart, Miss. Eva M...| 7.0|female|\n",
      "|Andersson, Miss. ...| 9.0|female|\n",
      "+--------------------+----+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Filter Operation\n",
    "df.filter((df['Age'] < 10) & (df['Sex'] == 'female')).select(['Name', 'Age', 'Sex']).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "a55a2eba-08a5-4a42-baa3-081c3c5fa4c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----------+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|      Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----------+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25|       NULL|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|        C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925|       NULL|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1|       C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05|       NULL|       S|\n",
      "|          7|       0|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|        E46|       S|\n",
      "|          9|       1|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333|       NULL|       S|\n",
      "|         10|       1|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708|       NULL|       C|\n",
      "|         12|       1|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55|       C103|       S|\n",
      "|         13|       0|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05|       NULL|       S|\n",
      "|         14|       0|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275|       NULL|       S|\n",
      "|         15|       0|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542|       NULL|       S|\n",
      "|         16|       1|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0|       NULL|       S|\n",
      "|         19|       0|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0|       NULL|       S|\n",
      "|         21|       0|     2|Fynney, Mr. Joseph J|  male|35.0|    0|    0|          239865|   26.0|       NULL|       S|\n",
      "|         22|       1|     2|Beesley, Mr. Lawr...|  male|34.0|    0|    0|          248698|   13.0|        D56|       S|\n",
      "|         23|       1|     3|\"McGowan, Miss. A...|female|15.0|    0|    0|          330923| 8.0292|       NULL|       Q|\n",
      "|         24|       1|     1|Sloper, Mr. Willi...|  male|28.0|    0|    0|          113788|   35.5|         A6|       S|\n",
      "|         26|       1|     3|Asplund, Mrs. Car...|female|38.0|    1|    5|          347077|31.3875|       NULL|       S|\n",
      "|         28|       0|     1|Fortune, Mr. Char...|  male|19.0|    3|    2|           19950|  263.0|C23 C25 C27|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(~(df['Age'] < 10)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c0a2c505-b756-419e-bd79-56f601974ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------------+-------------+-----------+--------+----------+----------+-----------------+\n",
      "|   Sex|sum(PassengerId)|sum(Survived)|sum(Pclass)|sum(Age)|sum(SibSp)|sum(Parch)|        sum(Fare)|\n",
      "+------+----------------+-------------+-----------+--------+----------+----------+-----------------+\n",
      "|female|          135343|          233|        678|  7286.0|       218|       204|13966.66279999999|\n",
      "|  male|          262043|          109|       1379|13919.17|       248|       136|14727.28649999999|\n",
      "+------+----------------+-------------+-----------+--------+----------+----------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Agregate function\n",
    "\n",
    "df.groupby('Sex').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6642f7b6-cc5b-41f9-b9e2-c79f3f50676d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+\n",
      "|   Sex|count|\n",
      "+------+-----+\n",
      "|female|  314|\n",
      "|  male|  577|\n",
      "+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count\n",
    "df.groupby('Sex').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59107416-a4ce-4ce4-ad42-a2e089e9df07",
   "metadata": {},
   "source": [
    "https://www.youtube.com/watch?v=_C8kWso4ne4"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.12",
   "language": "python",
   "name": "python3.12"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
